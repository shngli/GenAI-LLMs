{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b523e0a",
   "metadata": {},
   "source": [
    "# Lesson 4: Building a Multi-Document Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96713b38",
   "metadata": {
    "height": 30
   },
   "source": [
    "In Lesson 3, we built an agent that can reason over a single document and answer complex questions over it while maintaining memory. In Lesson 4, we will learn how to extend that agent to handle multiple documents in increasing degrees of complexity. We will start with a 3-document use case, and then we will expand to an 11 document use case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a323703",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b9625ab2-71b6-4fd0-904e-42df80d3215f",
   "metadata": {
    "height": 47,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from helper import get_openai_api_key\n",
    "OPENAI_API_KEY = get_openai_api_key()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3221a474-5817-4db2-af46-e029042a75a5",
   "metadata": {
    "height": 47,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20adaa26",
   "metadata": {},
   "source": [
    "## 1. Setup an agent over 3 papers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b71ff6",
   "metadata": {},
   "source": [
    "The first task is to set up our function calling agent over 3 papers. We do this by combining the vector summary tools for each document into a list and passing it to the agent, so that the agent actually has 6 tools in total. So we will download 3 papers from Eichler 2024, and convert each paper into a tool.\n",
    "\n",
    "**Note**: The pdf files are included with this lesson. To access these papers, go to the `File` menu and select`Open...`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ed10a24b-d65c-4b98-a93a-94ccdb8900d0",
   "metadata": {
    "height": 200,
    "tags": []
   },
   "outputs": [],
   "source": [
    "urls = [\n",
    "    \"https://openreview.net/pdf?id=VtmBAGCN7o\",\n",
    "    \"https://openreview.net/pdf?id=6PmJoRfdaK\",\n",
    "    \"https://openreview.net/pdf?id=hSyW5go0v8\",\n",
    "]\n",
    "\n",
    "papers = [\n",
    "    \"metagpt.pdf\",\n",
    "    \"longlora.pdf\",\n",
    "    \"selfrag.pdf\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0becce9",
   "metadata": {
    "height": 64
   },
   "source": [
    "In Lesson 3, we have a helper function called `get_doc_tools`, which automatically builds a vector index tool and a summary index tool over a given paper. So the vector tool performs vector search, and the summary tool performs summarization over the entire document. \n",
    "\n",
    "For each paper, we get back both the vector tool and summary tool, and we put it into this overall dictionary, mapping each paper name to the vector tool and summary tool. Next we simply get these tools in a flat list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0d8f3185-3221-4b00-bd38-41d36e4a3307",
   "metadata": {
    "height": 149,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting tools for paper: metagpt.pdf\n",
      "Getting tools for paper: longlora.pdf\n",
      "Getting tools for paper: selfrag.pdf\n"
     ]
    }
   ],
   "source": [
    "from utils import get_doc_tools\n",
    "from pathlib import Path\n",
    "\n",
    "paper_to_tools_dict = {}\n",
    "for paper in papers:\n",
    "    print(f\"Getting tools for paper: {paper}\")\n",
    "    vector_tool, summary_tool = get_doc_tools(paper, Path(paper).stem)\n",
    "    paper_to_tools_dict[paper] = [vector_tool, summary_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0e541bdd-14e1-41b6-81b5-b1bfda078d07",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [],
   "source": [
    "initial_tools = [t for paper in papers for t in paper_to_tools_dict[paper]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a25a74b",
   "metadata": {
    "height": 30
   },
   "source": [
    "We will define GPT 3.5 turbo from OpenAI as our LLM of choice. If we quickly take a look at the number of tools that are going to be passed to the agent, we will see that the number is 6. That's because we have 3 papers, and we have 2 tools for each paper: a vector tool and a summary tool.\n",
    "\n",
    "The next step is to construct our overall agent worker. And this agent work includes the 6 tools as well as the LLM that we parse. And now we are able to ask questions across these 3 documents or within a single document. For now, let's quickly ask a question about LongLoRA: `Tell me about the evaluation dataset used in LongLoRA, and then tell me about the evaluation results.` We get back the answer that one of the eval datasets used is the PG19 test split. And that we are able to look at the eval results for our LongLoRA models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bff58c52",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2f2c6a9f",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(initial_tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a124a438-5609-402e-8642-69d1088cb9ad",
   "metadata": {
    "height": 166,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.core.agent import FunctionCallingAgentWorker\n",
    "from llama_index.core.agent import AgentRunner\n",
    "\n",
    "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
    "    initial_tools, \n",
    "    llm=llm, \n",
    "    verbose=True\n",
    ")\n",
    "agent = AgentRunner(agent_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "17409d4c-05a9-4bf4-b74f-75135fa3cb6b",
   "metadata": {
    "height": 81,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Tell me about the evaluation dataset used in LongLoRA, and then tell me about the evaluation results\n",
      "=== Calling Function ===\n",
      "Calling function: vector_tool_longlora with args: {\"query\": \"evaluation dataset\"}\n",
      "=== Function Output ===\n",
      "PG19 test split\n",
      "=== Calling Function ===\n",
      "Calling function: vector_tool_longlora with args: {\"query\": \"evaluation results\"}\n",
      "=== Function Output ===\n",
      "The evaluation results include reporting perplexity for models and baselines on proof-pile and PG19 datasets, showing the effectiveness of the fine-tuning method with longer context sizes. The perplexity decreases as the context size increases, indicating improved performance. Additionally, experiments on retrieval in long contexts were conducted, comparing the model with other open LLMs on a topic retrieval task, showcasing promising results on extremely large settings.\n",
      "=== LLM Response ===\n",
      "The evaluation dataset used in LongLoRA includes the PG19 test split. \n",
      "\n",
      "As for the evaluation results, the study reported perplexity for models and baselines on proof-pile and PG19 datasets, demonstrating the effectiveness of the fine-tuning method with longer context sizes. The perplexity decreases as the context size increases, indicating improved performance. Furthermore, experiments on retrieval in long contexts were conducted, comparing the model with other open Large Language Models (LLMs) on a topic retrieval task, showing promising results on extremely large settings.\n"
     ]
    }
   ],
   "source": [
    "response = agent.query(\n",
    "    \"Tell me about the evaluation dataset used in LongLoRA, \"\n",
    "    \"and then tell me about the evaluation results\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc471e91",
   "metadata": {
    "height": 30
   },
   "source": [
    "The next question we can ask is `Give me a summary of both Self-RAG and LongLoRA`. So this allow us to do summarization across 2 papers. First we call the summary tool for selfrag with the input `Self-RAG`, and we will get back the output describing what the paper is about. The agent then calls a LongLoRA summary tool with the input `LongLoRA`, and then we get back an overall summary of LongLoRA. The final LLM response is that we are able to get back both a summary of selfrag and LongLoRA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ace340b1-761f-4058-be41-68cf131541e4",
   "metadata": {
    "height": 47,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Give me a summary of both Self-RAG and LongLoRA\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_selfrag with args: {\"input\": \"Self-RAG\"}\n",
      "=== Function Output ===\n",
      "Self-RAG is a framework that enhances the quality and factuality of a large language model by incorporating retrieval and self-reflection mechanisms. It allows the language model to adaptively retrieve passages on-demand, generate text, and reflect on both the retrieved passages and its own generations using special tokens called reflection tokens. This approach enables the language model to control its behavior during inference, tailoring it to diverse task requirements and has shown significant performance improvements over existing models in various tasks such as open-domain QA, reasoning, fact verification, and long-form generation.\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_longlora with args: {\"input\": \"LongLoRA\"}\n",
      "=== Function Output ===\n",
      "LongLoRA is an efficient method for extending the context sizes of pre-trained large language models (LLMs) with limited computation cost. It combines shifted sparse attention (S2-Attn) with LoRA to enable fine-tuning to longer context lengths while retaining the original model architectures. LongLoRA has shown strong empirical results on various tasks and is compatible with existing techniques like Flash-Attention2. It allows for extending LLMs' context while saving on computational resources and training time, making it a valuable tool for researchers working with large language models.\n",
      "=== LLM Response ===\n",
      "Self-RAG is a framework that enhances the quality and factuality of a large language model by incorporating retrieval and self-reflection mechanisms. It allows the language model to adaptively retrieve passages on-demand, generate text, and reflect on both the retrieved passages and its own generations using special tokens called reflection tokens. This approach enables the language model to control its behavior during inference, tailoring it to diverse task requirements and has shown significant performance improvements over existing models in various tasks such as open-domain QA, reasoning, fact verification, and long-form generation.\n",
      "\n",
      "LongLoRA is an efficient method for extending the context sizes of pre-trained large language models (LLMs) with limited computation cost. It combines shifted sparse attention (S2-Attn) with LoRA to enable fine-tuning to longer context lengths while retaining the original model architectures. LongLoRA has shown strong empirical results on various tasks and is compatible with existing techniques like Flash-Attention2. It allows for extending LLMs' context while saving on computational resources and training time, making it a valuable tool for researchers working with large language models.\n",
      "assistant: Self-RAG is a framework that enhances the quality and factuality of a large language model by incorporating retrieval and self-reflection mechanisms. It allows the language model to adaptively retrieve passages on-demand, generate text, and reflect on both the retrieved passages and its own generations using special tokens called reflection tokens. This approach enables the language model to control its behavior during inference, tailoring it to diverse task requirements and has shown significant performance improvements over existing models in various tasks such as open-domain QA, reasoning, fact verification, and long-form generation.\n",
      "\n",
      "LongLoRA is an efficient method for extending the context sizes of pre-trained large language models (LLMs) with limited computation cost. It combines shifted sparse attention (S2-Attn) with LoRA to enable fine-tuning to longer context lengths while retaining the original model architectures. LongLoRA has shown strong empirical results on various tasks and is compatible with existing techniques like Flash-Attention2. It allows for extending LLMs' context while saving on computational resources and training time, making it a valuable tool for researchers working with large language models.\n"
     ]
    }
   ],
   "source": [
    "response = agent.query(\"Give me a summary of both Self-RAG and LongLoRA\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eede70c",
   "metadata": {},
   "source": [
    "## 2. Setup an agent over 11 papers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fde7af",
   "metadata": {
    "height": 64
   },
   "source": [
    "If we want to try out some queries on our own, we can try out any combinations of these 2 or even 3 papers, and ask for both summaries as well as specific information within the papers, to see whether or not the agent is able to reason about the summary and vector tools for each document. \n",
    "\n",
    "Let's expand into a more advanced use case, using 11 research papers from Eichler 2024."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18771e69",
   "metadata": {},
   "source": [
    "### Download 11 ICLR papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "60d01d2c-547f-4054-b0fe-ed9b1a9cc3b5",
   "metadata": {
    "height": 472,
    "tags": []
   },
   "outputs": [],
   "source": [
    "urls = [\n",
    "    \"https://openreview.net/pdf?id=VtmBAGCN7o\",\n",
    "    \"https://openreview.net/pdf?id=6PmJoRfdaK\",\n",
    "    \"https://openreview.net/pdf?id=LzPWWPAdY4\",\n",
    "    \"https://openreview.net/pdf?id=VTF8yNQM66\",\n",
    "    \"https://openreview.net/pdf?id=hSyW5go0v8\",\n",
    "    \"https://openreview.net/pdf?id=9WD9KwssyT\",\n",
    "    \"https://openreview.net/pdf?id=yV6fD7LYkF\",\n",
    "    \"https://openreview.net/pdf?id=hnrB5YHoYu\",\n",
    "    \"https://openreview.net/pdf?id=WbWtOYIzIK\",\n",
    "    \"https://openreview.net/pdf?id=c5pwL0Soay\",\n",
    "    \"https://openreview.net/pdf?id=TpD2aG1h0D\"\n",
    "]\n",
    "\n",
    "papers = [\n",
    "    \"metagpt.pdf\",\n",
    "    \"longlora.pdf\",\n",
    "    \"loftq.pdf\",\n",
    "    \"swebench.pdf\",\n",
    "    \"selfrag.pdf\",\n",
    "    \"zipformer.pdf\",\n",
    "    \"values.pdf\",\n",
    "    \"finetune_fair_diffusion.pdf\",\n",
    "    \"knowledge_card.pdf\",\n",
    "    \"metra.pdf\",\n",
    "    \"vr_mcl.pdf\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c550a0f6",
   "metadata": {
    "height": 166
   },
   "source": [
    "We will build a dictionary mapping each paper to its vector and summary tool. This section can also take a little bit of time, since we need to process, index and embed 11 documents. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77426cb",
   "metadata": {
    "tags": []
   },
   "source": [
    "To download these papers, below is the needed code:\n",
    "\n",
    "\n",
    "    #for url, paper in zip(urls, papers):\n",
    "         #!wget \"{url}\" -O \"{paper}\"\n",
    "    \n",
    "    \n",
    "**Note**: The pdf files are included with this lesson. To access these papers, go to the `File` menu and select`Open...`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ea5ee34d-02ac-4537-ae20-7ef6c5767172",
   "metadata": {
    "height": 149,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting tools for paper: metagpt.pdf\n",
      "Getting tools for paper: longlora.pdf\n",
      "Getting tools for paper: loftq.pdf\n",
      "Getting tools for paper: swebench.pdf\n",
      "Getting tools for paper: selfrag.pdf\n",
      "Getting tools for paper: zipformer.pdf\n",
      "Getting tools for paper: values.pdf\n",
      "Getting tools for paper: finetune_fair_diffusion.pdf\n",
      "Getting tools for paper: knowledge_card.pdf\n",
      "Getting tools for paper: metra.pdf\n",
      "Getting tools for paper: vr_mcl.pdf\n"
     ]
    }
   ],
   "source": [
    "from utils import get_doc_tools\n",
    "from pathlib import Path\n",
    "\n",
    "paper_to_tools_dict = {}\n",
    "for paper in papers:\n",
    "    print(f\"Getting tools for paper: {paper}\")\n",
    "    vector_tool, summary_tool = get_doc_tools(paper, Path(paper).stem)\n",
    "    paper_to_tools_dict[paper] = [vector_tool, summary_tool]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e35d52c",
   "metadata": {},
   "source": [
    "### Extend the Agent with Tool Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002daead",
   "metadata": {
    "height": 132
   },
   "source": [
    "Now let's collapse these tools into a flat list. This is the point at which we need a slightly more advanced agent and tool architecture. The issue is that if we try to index all 11 papers, which now includes 20 tools, or if we try to index 100 papers or more, even though LLM context windows are getting longer, stuffing too many tool selections into the LLM prompt leads to the following issues:\n",
    "\n",
    "    1. The tools may not all fit in the prompt, especially if our number of documents are big and we are modeling each document as a separate tool or a set of tools. Costs and latency will spike because we are increasing the number of tokens in our prompt. \n",
    "\n",
    "    2. The LLM can actually get confused. The LLM may fail to pick the right tool when the number of choices is too large. \n",
    "\n",
    "A solution is that when the user asks a query, we actually perform Retrieval Augmentation, but not on the level of text, but actually the level of tools. We first retrieve a small set of relevant tools, and then feed the relevant tools to the agent reasoning prompt instead of all the tools. This retrieval process is similar to the retrieval process used in RAG. At its simplest, it can just be top-K vector search. But we can also add all the advanced retrieval techniques we want, to filter out the relevant set of results. Our agents let us plug in a tool retriever that allows us to accomplish this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14cdff5",
   "metadata": {
    "height": 98
   },
   "source": [
    "So let's get this done. First we want to index the tools. LlamaIdex already has extensive indexing capabilities over general text documents. Since these tools are actually Python objects, we need to convert and serialize these objects to a string representation and back. This is solved through the object index abstraction in LlamaIndex. \n",
    "\n",
    "So we will define an object index and retriever over these tools. We import `VectorStoreIndex`, which is our standard interface for indexing text. Then we wrap `VectorStoreIndex` with `ObjectIndex`. And to construct an object index, we directly plug in these Python tools as input into the index. \n",
    "\n",
    "We can retrieve from an object index through an object retriever. This will call the underlying retriever from the index, and return the output directly as objects. In this case, it will be tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "20154923-873e-4941-9a3a-4926ab5f9b8c",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_tools = [t for paper in papers for t in paper_to_tools_dict[paper]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "671582f9-70d7-4a8f-b813-58b2a068ca72",
   "metadata": {
    "height": 149,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define an \"object\" index and retriever over these tools\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.objects import ObjectIndex\n",
    "\n",
    "obj_index = ObjectIndex.from_objects(\n",
    "    all_tools,\n",
    "    index_cls=VectorStoreIndex,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c3929882-e9dc-46ca-b495-53e3ed60340e",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [],
   "source": [
    "obj_retriever = obj_index.as_retriever(similarity_top_k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3480faf4",
   "metadata": {
    "height": 64
   },
   "source": [
    "Now that we have defined the object retriever, let's walk through a very simple example. Let's ask `Tell me about the eval dataset used in MetaGPT and SWE-Bench`. We look at the first tool in this list. We see that we actually directly retrieved a set of tools, and that the first tool is the summary tool for MetaGPT (`summary_tool_metagpt`). \n",
    "\n",
    "If we look at the second tool, we see that this is a summary tool for an unrelated paper to MetaGPT and Swebench, so the quality of retrieval is dependent on our embedding model. However, we see that the last tool that's rtrieved is indeed the summary tool for Swebench (`summary_tool_swebench`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ba9cfecd-fe14-4da8-b9ba-b3d485d98a03",
   "metadata": {
    "height": 64,
    "tags": []
   },
   "outputs": [],
   "source": [
    "tools = obj_retriever.retrieve(\n",
    "    \"Tell me about the eval dataset used in MetaGPT and SWE-Bench\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c270ffbf-69c7-48ea-a028-9ba25221cde5",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ToolMetadata(description='Useful for summarization questions related to metagpt', name='summary_tool_metagpt', fn_schema=<class 'llama_index.core.tools.types.DefaultToolFnSchema'>, return_direct=False)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "09eeb1ba",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ToolMetadata(description='Useful for summarization questions related to metra', name='summary_tool_metra', fn_schema=<class 'llama_index.core.tools.types.DefaultToolFnSchema'>, return_direct=False)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools[1].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "21a5a7f5",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ToolMetadata(description='Useful for summarization questions related to swebench', name='summary_tool_swebench', fn_schema=<class 'llama_index.core.tools.types.DefaultToolFnSchema'>, return_direct=False)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools[2].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2b4388",
   "metadata": {
    "height": 30
   },
   "source": [
    "Now we are ready to set up our function calling agent. We note that the setup is pretty similar to the setup in Lesson 3. However, just as an additional feature, we can actually add a system prompt to the agent if we want. This is optional, we don't need to specify this, but we can if we want an additional guidance to prompt the agent to output things in a certain way, or if we want it to take into account certain factors when it reasons over those tools. So this is an example of that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9cc0a0b6-9858-4348-9ae0-1cd4160f3fb7",
   "metadata": {
    "height": 251,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.core.agent import FunctionCallingAgentWorker\n",
    "from llama_index.core.agent import AgentRunner\n",
    "\n",
    "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
    "    tool_retriever=obj_retriever,\n",
    "    llm=llm, \n",
    "    system_prompt=\"\"\" \\\n",
    "You are an agent designed to answer queries over a set of given papers.\n",
    "Please always use the tools provided to answer a question. Do not rely on prior knowledge.\\\n",
    "\n",
    "\"\"\",\n",
    "    verbose=True\n",
    ")\n",
    "agent = AgentRunner(agent_worker)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4eb77b7",
   "metadata": {
    "height": 30
   },
   "source": [
    "Now let's try asking some comparion queries. We ask `Tell me about the evaluation dataset used in MetaGPT and compare it against SWE-Bench`. We see that it calls both the summary tool for MetaGPT and the summary tool for Swebench. It is able to get back results for both. And then it generates a final response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a250cf1a-e011-4994-bcca-4e0294f20864",
   "metadata": {
    "height": 98,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Tell me about the evaluation dataset used in MetaGPT and compare it against SWE-Bench\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_metagpt with args: {\"input\": \"evaluation dataset used in MetaGPT\"}\n",
      "=== Function Output ===\n",
      "The evaluation dataset used in MetaGPT includes three benchmarks: HumanEval, MBPP, and SoftwareDev. HumanEval consists of 164 handwritten programming tasks, MBPP comprises 427 Python tasks, and SoftwareDev is a collection of 70 representative software development tasks covering various scopes like mini-games, image processing algorithms, and data visualization.\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_swebench with args: {\"input\": \"evaluation dataset used in SWE-Bench\"}\n",
      "=== Function Output ===\n",
      "The evaluation dataset used in SWE-Bench consists of task instances collected from real GitHub repositories, including popular Python repositories. It includes issues, pull requests, task instructions, retrieved files, documentation, example patch files, and prompts for generating patch files. The dataset aims to provide realistic and challenging scenarios for evaluating language models in software engineering tasks. Task instances are validated through execution-based validation to ensure usability, with a focus on resolving issues, patch generation, reasoning over contexts, navigating codebases, and capturing dependency-based relationships. The dataset is constructed by scraping pull requests from top PyPI libraries, converting them into task instances with components like codebases, problem statements, tests, and solutions. Task instances are continuously updated, and finalized instances undergo validation to ensure successful application, installation, and execution on codebases. The dataset also includes information on test-to-status mapping, patch generations, model performance comparisons, and statistics on patch outcomes.\n",
      "=== LLM Response ===\n",
      "The evaluation dataset used in MetaGPT includes three benchmarks: HumanEval, MBPP, and SoftwareDev. HumanEval consists of 164 handwritten programming tasks, MBPP comprises 427 Python tasks, and SoftwareDev is a collection of 70 representative software development tasks covering various scopes like mini-games, image processing algorithms, and data visualization.\n",
      "\n",
      "On the other hand, the evaluation dataset used in SWE-Bench consists of task instances collected from real GitHub repositories, including popular Python repositories. It includes issues, pull requests, task instructions, retrieved files, documentation, example patch files, and prompts for generating patch files. The dataset aims to provide realistic and challenging scenarios for evaluating language models in software engineering tasks. Task instances are validated through execution-based validation to ensure usability, with a focus on resolving issues, patch generation, reasoning over contexts, navigating codebases, and capturing dependency-based relationships. The dataset is constructed by scraping pull requests from top PyPI libraries, converting them into task instances with components like codebases, problem statements, tests, and solutions. Task instances are continuously updated, and finalized instances undergo validation to ensure successful application, installation, and execution on codebases. The dataset also includes information on test-to-status mapping, patch generations, model performance comparisons, and statistics on patch outcomes.\n",
      "assistant: The evaluation dataset used in MetaGPT includes three benchmarks: HumanEval, MBPP, and SoftwareDev. HumanEval consists of 164 handwritten programming tasks, MBPP comprises 427 Python tasks, and SoftwareDev is a collection of 70 representative software development tasks covering various scopes like mini-games, image processing algorithms, and data visualization.\n",
      "\n",
      "On the other hand, the evaluation dataset used in SWE-Bench consists of task instances collected from real GitHub repositories, including popular Python repositories. It includes issues, pull requests, task instructions, retrieved files, documentation, example patch files, and prompts for generating patch files. The dataset aims to provide realistic and challenging scenarios for evaluating language models in software engineering tasks. Task instances are validated through execution-based validation to ensure usability, with a focus on resolving issues, patch generation, reasoning over contexts, navigating codebases, and capturing dependency-based relationships. The dataset is constructed by scraping pull requests from top PyPI libraries, converting them into task instances with components like codebases, problem statements, tests, and solutions. Task instances are continuously updated, and finalized instances undergo validation to ensure successful application, installation, and execution on codebases. The dataset also includes information on test-to-status mapping, patch generations, model performance comparisons, and statistics on patch outcomes.\n"
     ]
    }
   ],
   "source": [
    "response = agent.query(\n",
    "    \"Tell me about the evaluation dataset used \"\n",
    "    \"in MetaGPT and compare it against SWE-Bench\"\n",
    ")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784c7949",
   "metadata": {
    "height": 98
   },
   "source": [
    "As a final example, let's compare and contrast the 2 Lora papers, LongLoRA and LoftQ, and analyze the approach in each paper first. We see that the agent is executing this query, and the first step it takes is this input task and actually retrieves the set of input tools that help it fulfill this task. So through the object retriever, the expectation is that it actually retrieves LongLoRA and LoftQ query tools in order to help it fulfill its response. \n",
    "\n",
    "If we take a look at the intermediate outputs of the agent, we see that it is able to have access to relevant tools from LongLoRA and LoftQ. We see that it first calls `summary_tool_longlora` with the arguments `Approach in LongLoRA`, and we are able to get back a summary of the approach. Similarly, we are able to get back the approach in LoftQ by calling `summary_tool_loftq`. \n",
    "\n",
    "The final LLM response is able to compare these 2 approaches by comparing the responses from these 2 tools, and combining them to synthesize an answer that satisfies the user query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8084c8cb-98ed-4835-aaa4-5b0c7254be6d",
   "metadata": {
    "height": 81,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Compare and contrast the LoRA papers (LongLoRA, LoftQ). Analyze the approach in each paper first. \n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_longlora with args: {\"input\": \"Approach in LongLoRA\"}\n",
      "=== Function Output ===\n",
      "The approach in LongLoRA involves efficiently extending the context length of large language models (LLMs) to significantly larger sizes while saving on computational costs during fine-tuning. It focuses on maintaining the quality of the original attention architecture during inference and emphasizes efficient adaptation of LLMs to longer context lengths by incorporating trainable normalization and embedding layers. Additionally, LongLoRA utilizes methods like S2-Attn, Flash-Attention2, and DeepSpeed during fine-tuning to achieve promising results on extremely large context settings and handle long documents effectively.\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_loftq with args: {\"input\": \"Approach in LoftQ\"}\n",
      "=== Function Output ===\n",
      "The approach in LoftQ involves integrating quantization and low-rank approximation techniques to approximate the original high-precision pre-trained weights. This framework aims to optimize the initial values of the quantized backbone and low-rank adapters through an alternating optimization process, enhancing generalization in downstream tasks. By applying quantization, low-rank approximation, and iterative optimization methods, LoftQ effectively reduces the memory footprint and computational cost of neural network models while maintaining performance levels close to full finetuning.\n",
      "=== LLM Response ===\n",
      "The approach in LongLoRA focuses on efficiently extending the context length of large language models (LLMs) to significantly larger sizes while saving on computational costs during fine-tuning. It emphasizes maintaining the quality of the original attention architecture during inference and incorporates trainable normalization and embedding layers for efficient adaptation of LLMs to longer context lengths. LongLoRA also utilizes methods like S2-Attn, Flash-Attention2, and DeepSpeed during fine-tuning to achieve promising results on extremely large context settings and handle long documents effectively.\n",
      "\n",
      "On the other hand, the approach in LoftQ involves integrating quantization and low-rank approximation techniques to approximate the original high-precision pre-trained weights. This framework optimizes the initial values of the quantized backbone and low-rank adapters through an alternating optimization process to enhance generalization in downstream tasks. By applying quantization, low-rank approximation, and iterative optimization methods, LoftQ effectively reduces the memory footprint and computational cost of neural network models while maintaining performance levels close to full finetuning.\n"
     ]
    }
   ],
   "source": [
    "response = agent.query(\n",
    "    \"Compare and contrast the LoRA papers (LongLoRA, LoftQ). \"\n",
    "    \"Analyze the approach in each paper first. \"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88abe4c7",
   "metadata": {
    "height": 30
   },
   "source": [
    "So that concludes our lesson. Now we should be equipped with the right tools to build agents over a single document and also over multiple documents. This would enable us to build more general, complex context-augmented research assistance that can answer complex questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f92e406",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
