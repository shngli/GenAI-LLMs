{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd7c4c87",
   "metadata": {},
   "source": [
    "# Lesson 1: Router Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877df988",
   "metadata": {},
   "source": [
    "Welcome to Lesson 1.\n",
    "\n",
    "To access the `requirements.txt` file, the data/pdf file required for this lesson and the `helper` and `utils` modules, please go to the `File` menu and select`Open...`.\n",
    "\n",
    "I hope you enjoy this course!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357c97c9",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567f4533",
   "metadata": {
    "height": 30
   },
   "source": [
    "The first thing we will import our OpenAI key with a helper function. Next we will import `nest_asyncio`, because Jupyter runs an event loop behind the scenes, and a lot of our modules use async. We need to import `nest_asyncio` to make async play nice with Jupyter notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab5f4f4a-5890-451c-8869-24606ef9f396",
   "metadata": {
    "height": 64,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from helper import get_openai_api_key\n",
    "\n",
    "OPENAI_API_KEY = get_openai_api_key()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffc9b4f4-64d4-4266-9889-54db90e00ee9",
   "metadata": {
    "height": 64,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49fca250",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ae2a8c",
   "metadata": {
    "tags": []
   },
   "source": [
    "The next step to load a sample document, MetaGPT.pdf (a research paper). To download this paper, below is the needed code:\n",
    "\n",
    "#!wget \"https://openreview.net/pdf?id=VtmBAGCN7o\" -O metagpt.pdf\n",
    "\n",
    "**Note**: The pdf file is included with this lesson. To access it, go to the `File` menu and select`Open...`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c7f012d-dcd3-4881-a568-72dd27d79159",
   "metadata": {
    "height": 81,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "# load documents\n",
    "documents = SimpleDirectoryReader(input_files=[\"metagpt.pdf\"]).load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b48a301",
   "metadata": {},
   "source": [
    "## Define LLM and Embedding model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9685d2",
   "metadata": {
    "height": 98
   },
   "source": [
    "Next we will import the sentence splitter from LlamaIndex. We will use our simple directory reader module and LlamaIndex to read the PDF into a parsed document representation. To split these documents into even-sized chunks, and we will split on the order of sentences. So we set `chunk_size=1024`, and we call `splitter.get_nodes_from_documents()` to split these documents into nodes.\n",
    "\n",
    "The next step is optional, and this allows us to find an LLM and embedding model. We can do this by specifying a global config setting where we specify the LLM and embedding mode that we want to inject as part of the global config. By default, we use `GPT 3.5 turbo` and `text embedding ada 2.0` in this course.\n",
    "\n",
    "We define the settings object, and `Settings.llm = OpenAi()`, and `Settings.embed_model = OpenAIEmbedding()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a537bc0-78ee-4dda-a43f-60fd80062df6",
   "metadata": {
    "height": 81,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "splitter = SentenceSplitter(chunk_size=1024)\n",
    "nodes = splitter.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de0660ee-b231-4351-b158-d8ad023e00b5",
   "metadata": {
    "height": 115,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "Settings.llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
    "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-ada-002\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997c7559",
   "metadata": {},
   "source": [
    "## Define Summary Index and Vector Index over the Same Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b36fa5d",
   "metadata": {
    "height": 132
   },
   "source": [
    "Now we will start building some indexes. We define 2 indexes over these nodes. This includes both a summary index and a vector index. We can think of an index as a set of metadata over our data. We can query an index, and different indexes will have different retrieval behaviors. \n",
    "\n",
    "A vector index indexes nodes via text embeddings and its core abstraction in LlamaIndex, and a core abstraction for building any sort of RAG system. Querying a vector index will return the most similar nodes by embedding similarity.\n",
    "\n",
    "A summary index, on the other hand, is also a very simple index, but querying it will return all the nodes currently in the index, so it doesn't necessarily depend on the user query, but will return all the nodes that is currently in the index. \n",
    "\n",
    "To set up both the summary and vector index, we will just import these two simple modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73d01b01-bc74-432a-8d92-07b9e86498b0",
   "metadata": {
    "height": 81,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.core import SummaryIndex, VectorStoreIndex\n",
    "\n",
    "summary_index = SummaryIndex(nodes)\n",
    "vector_index = VectorStoreIndex(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9898d3f",
   "metadata": {},
   "source": [
    "## Define Query Engines and Set Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66fa059",
   "metadata": {
    "height": 30
   },
   "source": [
    "Now let's turn these indexes into query entrants and then query tools. Each query entrant represents an overall query interface over the data that's stored in this index, and combines retrieval with LLM synthesis. Each query entrant is good for a certain type of question, and this is a great use case for a router, which can route dynamically between these different query entrants.\n",
    "\n",
    "A query tool now is just the query entrant with metadata, specifically a description of what types of questions the tool can answer. So here we define `summary_query_engine = summary_index.as_query_engine()`. And then also `vector_query_engine = vector_index.as_query_engine()`. \n",
    "\n",
    "We can see that the query engine is derived from each of the indexes. We can see that for the summary query engine, we set `use_async=True` to enforce fast query generation by leveraging async capabilities.\n",
    "\n",
    "Next, a query tool is just a query engine with metadata. It is specifically a description of what types of questions the tool can answer. We will define a query tool for both the summary and vector query engines. Through this code snippet here, we see that the summary tool description is useful for summarization questions related to MetaGPT, and the vector tool description is useful for retrieving specific context from the MetaGPT paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44cd7046-c714-4920-b077-b3ded917862f",
   "metadata": {
    "height": 98,
    "tags": []
   },
   "outputs": [],
   "source": [
    "summary_query_engine = summary_index.as_query_engine(\n",
    "    response_mode=\"tree_summarize\",\n",
    "    use_async=True,\n",
    ")\n",
    "vector_query_engine = vector_index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a1d6d75-247e-426a-8ef4-b49225c24796",
   "metadata": {
    "height": 268,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.core.tools import QueryEngineTool\n",
    "\n",
    "summary_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=summary_query_engine,\n",
    "    description=(\n",
    "        \"Useful for summarization questions related to MetaGPT\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "vector_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=vector_query_engine,\n",
    "    description=(\n",
    "        \"Useful for retrieving specific context from the MetaGPT paper.\"\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d2c152",
   "metadata": {},
   "source": [
    "## Define Router Query Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59044dec",
   "metadata": {
    "height": 30
   },
   "source": [
    "Now that we have our query engines and tools, we are ready to define our router. LlamaIndex provides several different types of selectors to enable us to build a router, and each of these selectors has distinct attributes. \n",
    "\n",
    "The LLM selector is one option, and it involves prompting an LLM to output a json that is then parsed, and then the corresponding indexes are queried.\n",
    "\n",
    "Another option is to use the Pydantic selectors. Instead of directly prompting the LLM with text, we actually use the function calling APIs supported by models like OpenAI to produce Pydantic selection objects, rather than parsing raw json. \n",
    "\n",
    "For each of these types of selectors, we also have the dynamic capabilities to select one index to route two, or actually multiple.\n",
    "\n",
    "Let's try an LLM-powered single selector called `LLMSingleSelector`. So we import 2 modules: a `RouterQueryEngine` and the `LLMSingleSelector`. We see that the `RouterQueryEngine` takes in a `selector` type and a set of `query_engine_tools`. The `selector` type is just the `LLMSingleSelector()`, which means that it prompts the LLM and makes a single selection. The `query_engine_tools` include the summarization tool and the vector tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00734d7c-638a-4d63-ab1f-7f5a92a65119",
   "metadata": {
    "height": 200,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine.router_query_engine import RouterQueryEngine\n",
    "from llama_index.core.selectors import LLMSingleSelector\n",
    "\n",
    "query_engine = RouterQueryEngine(\n",
    "    selector=LLMSingleSelector.from_defaults(),\n",
    "    query_engine_tools=[\n",
    "        summary_tool,\n",
    "        vector_tool,\n",
    "    ],\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f096de7",
   "metadata": {
    "height": 98
   },
   "source": [
    "Let's now test some queries. The first question we will ask is \"What is a summary of the document?\" \n",
    "\n",
    "The verbose output allows us to view the intermediate steps that are being taken. We see that the output includes `Selecting query engine 0: Useful for summarization questions related to MetaGPT`. This means that the first option, the summary tool, is actually picked in order to help answer this question. As a result, we are able to get back a response. The document introduces MetaGPT, a meta-programming framework for LLM-based multi-agent collaboration. And this gives an overall summary of the paper, and it is synthesized over all the context in the paper. \n",
    "\n",
    "So the response comes with sources. And to inspect the sources, we can take a look at `response.source_nodes`. We see that the length of `response.source_nodes` is equal to 34. Coincidentally, that is exactly equal to the number of chunks of the entire document. And so we see that the summary query engine must have been getting called, because the summary query engine returns all the chunks corresponding to the items within its index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe3f0a76-68a8-444d-867f-d084bb3ff112",
   "metadata": {
    "height": 47,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;5;200mSelecting query engine 0: Useful for summarization questions related to MetaGPT.\n",
      "\u001b[0mThe document introduces MetaGPT, a meta-programming framework that enhances multi-agent collaboration using Large Language Models (LLMs) in software development. It incorporates Standardized Operating Procedures (SOPs) to streamline workflows, assign specialized roles to agents, and improve communication. MetaGPT achieves state-of-the-art performance in code generation benchmarks, offers an executable feedback mechanism for debugging and executing code during runtime, and emphasizes the importance of role specialization and efficient communication mechanisms. The framework presents a promising approach to developing efficient LLM-based multi-agent systems with a focus on high-quality results and iterative code improvement.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"What is the summary of the document?\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3fedea0-f2a9-46bb-8aaf-287df65b8fff",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n"
     ]
    }
   ],
   "source": [
    "print(len(response.source_nodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b1027b",
   "metadata": {
    "height": 64
   },
   "source": [
    "Let's take a look at another example. We will ask the question: \"How do agents share information with other agents?\" So let's ask this against the overall router query engine, and take a look at both the verbose output as well as the response. \n",
    "\n",
    "Here, we see that we actually select query engine 1, and the LLM gives some reasoning as to why it actually picks the vector search tool as opposed to the summary tool. It is because the focus is on retrieving specific context from the MetaGPT paper, where agents sharing information with other agents is probably located within a paragraph of that paper. And it's able to find that context and generate a response. So it is able to utilize a shared message pool where they can publish structured messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af8c31b3-8e22-4ad9-9825-b8de21bd03c0",
   "metadata": {
    "height": 81,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;5;200mSelecting query engine 1: This choice is more relevant as it focuses on retrieving specific context from the MetaGPT paper, which may contain information on how agents share information with other agents..\n",
      "\u001b[0mAgents share information with other agents by utilizing a shared message pool where they can publish structured messages. This shared message pool allows all agents to exchange messages directly, enabling them to both publish their own messages and access messages from other agents transparently. Additionally, agents can subscribe to relevant messages based on their role profiles, ensuring that they receive only the information that is pertinent to their tasks and responsibilities.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\n",
    "    \"How do agents share information with other agents?\"\n",
    ")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed060ee",
   "metadata": {},
   "source": [
    "## Let's put everything together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106c2d69",
   "metadata": {
    "height": 98
   },
   "source": [
    "And that basically helps to conclude Lesson 1. To put everything together, all the code above can be consolidated into a single helper function that takes in a file path and builds the router query engine with both vector search and summarization over it. \n",
    "\n",
    "So this is an `utils` module called `get_router_query_engine`. We can use it to query MetaGPT PDF, and test an example question \"Tell me about the ablation study results.\" And we will get back the response from the query engine. \n",
    "\n",
    "In this case, we also look at query engine 1 because the ablation study results reference specific context from the MetaGPT paper, so we want to do a vector search. And we are able to get back a final answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d8f92e0b-1c54-489b-b8dd-41ebaafb380a",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "from utils import get_router_query_engine\n",
    "\n",
    "query_engine = get_router_query_engine(\"metagpt.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ec1a43f3-77dc-472a-8adc-56551c00a0ff",
   "metadata": {
    "height": 47,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;5;200mSelecting query engine 1: Ablation study results are specific context from the MetaGPT paper, making choice 2 the most relevant..\n",
      "\u001b[0mThe ablation study results show that MetaGPT effectively addresses challenges related to context utilization, code hallucinations, and information overload in software development. By accurately unfolding natural language descriptions and maintaining information validity, MetaGPT eliminates ambiguity and allows Language Models (LLMs) to focus on relevant data. Additionally, by focusing on granular tasks like requirement analysis and package selection, MetaGPT guides LLMs in software generation, reducing code hallucination issues. Furthermore, MetaGPT uses a global message pool and subscription mechanism to tackle information overload, ensuring efficient communication and filtering out irrelevant contexts to enhance the relevance and utility of information.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"Tell me about the ablation study results?\")\n",
    "print(str(response))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
