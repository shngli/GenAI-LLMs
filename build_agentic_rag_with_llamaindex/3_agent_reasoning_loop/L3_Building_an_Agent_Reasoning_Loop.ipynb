{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64ec2f0f",
   "metadata": {},
   "source": [
    "# Lesson 3: Building an Agent Reasoning Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3520597d",
   "metadata": {
    "height": 64
   },
   "source": [
    "So far, our queries have been done in a single forward pass. Given the query, call the right tool with the right parameters, and get back the response. But this is still quite limiting. What if the user asks a complex question consisting of multiple steps, or a vague question that needs clarification? In this lesson, we will define a complete agent reasoning loop. Instead of tool calling in a single-shot setting, an agent is able to reason over tools in multiple steps. \n",
    "\n",
    "We will use the function calling agent implementation, which is an agent that natively integrates with the function calling capabilities of LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d7f1cf",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b07baa43-7a51-4c39-91cc-aa0d9619b69f",
   "metadata": {
    "height": 47,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from helper import get_openai_api_key\n",
    "OPENAI_API_KEY = get_openai_api_key()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dcfa86a3-c7f2-41fa-b8b6-5617659ec36a",
   "metadata": {
    "height": 47,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3af4bb",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8bfb34",
   "metadata": {
    "tags": []
   },
   "source": [
    "To download this paper, below is the needed code:\n",
    "\n",
    "#!wget \"https://openreview.net/pdf?id=VtmBAGCN7o\" -O metagpt.pdf\n",
    "\n",
    "**Note**: The pdf file is included with this lesson. To access it, go to the `File` menu and select`Open...`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb741560",
   "metadata": {},
   "source": [
    "## Setup the Query Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0726d0",
   "metadata": {
    "height": 30
   },
   "source": [
    "We will use the same MetaGPT paper, and we will also set up the auto-retrieval vector search tool and the summarization tool from Lesson 2. To make this more concise, we have packaged this into `get_doc_tools()` that we can import from the `utils` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77464fb2-5ace-4839-9032-a020df8d4259",
   "metadata": {
    "height": 64,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils import get_doc_tools\n",
    "\n",
    "vector_tool, summary_tool = get_doc_tools(\"metagpt.pdf\", \"metagpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40aae3fc",
   "metadata": {},
   "source": [
    "## Setup Function Calling Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460246ca",
   "metadata": {
    "height": 30
   },
   "source": [
    "We now set up our function calling agent. We use GPT 3.5 turbo as our LLM. We will then define our agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff4f5199-d02c-47b0-a9ab-cf72c8a506a3",
   "metadata": {
    "height": 64,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69362343",
   "metadata": {
    "height": 30
   },
   "source": [
    "So we import `FunctionCallingAgentWorker` and `AgentRunner` from LlamaIndex. And for `FunctionCallingAgentWorker()`, we pass in 2 set of tools: `vector_tool` and `summary_tool`. We also pass in the LLM and set `verbose=True` to look at the intermediate outputs.\n",
    "\n",
    "Think about `FunctionCallingAgentWorker` primary responsibility as given the existing conversation history, memory and any passed state along with the current user input. Use function calling to decide the next protocol, call that tool, and decide whether or not to return a final response. The overall agent interface is behind the agent runner, and that's what we are going to use to query the agent.\n",
    "\n",
    "We will first ask this question: `Tell me about the agent roles in MetaGPT, and then how they communicate with each other.` \n",
    "\n",
    "So let's trace through the outputs of this agent. We see that the agent is able to break down this overall question into steps. So the first part of the question is asking about agent roles and MetaGPT, and it calls the summary tool to answer this question. Now a quick note is that the summary tool isn't necessarily the most precise. We could argue that the vector tool will actually give us back a more concise set of context that better represents this relevant pieces of text that we are looking for. However, a summary tool is still a reasonable tool for the job. And of coure, more powerful models like Turbo 4 or Claude 3 Opus & Sonnet might be able to pick the more precise vector tool to help answer this question. In any case, we are able to get back the output: `Agent roles in MetaGPT include Product Manager, Architect, Project Manager, Engineer, and QA Engineer.`\n",
    "\n",
    "And then it uses this to perform chain-of-throught to then trigger the next question, which is `communication between agent roles in MetaGPT`. We are able to get back an answer about that too: `Communication between agent roles in MetaGPT is structured and efficient...`, and we are able to combine this entire conversation history to generate a final response: `In MetaGPT, the agent roles include.... They work together in a sequential workflow following Standard Operating Procedures...`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9365d78d-8e9f-4f22-8d57-35a4c6aa6baf",
   "metadata": {
    "height": 166,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.core.agent import FunctionCallingAgentWorker\n",
    "from llama_index.core.agent import AgentRunner\n",
    "\n",
    "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
    "    [vector_tool, summary_tool], \n",
    "    llm=llm, \n",
    "    verbose=True\n",
    ")\n",
    "agent = AgentRunner(agent_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a9535d7-0baf-4905-ad16-5fb903d33b85",
   "metadata": {
    "height": 81,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Tell me about the agent roles in MetaGPT, and then how they communicate with each other.\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_metagpt with args: {\"input\": \"agent roles in MetaGPT\"}\n",
      "=== Function Output ===\n",
      "Agent roles in MetaGPT include Product Manager, Architect, Project Manager, Engineer, and QA Engineer. The Product Manager is responsible for analyzing user requirements and formulating a detailed Product Requirement Document (PRD). The Architect translates these requirements into system design components. The Project Manager distributes tasks based on the system design, while Engineers execute designated classes and functions. The QA Engineer formulates test cases to ensure code quality. These roles work together in a sequential workflow following Standard Operating Procedures (SOPs) to efficiently develop software solutions in MetaGPT.\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_metagpt with args: {\"input\": \"how agents communicate with each other in MetaGPT\"}\n",
      "=== Function Output ===\n",
      "Agents in MetaGPT communicate with each other through structured communication interfaces, utilizing a shared message pool for exchanging information directly. They also employ a subscription mechanism based on their role-specific interests to extract relevant information, ensuring effective communication and task execution. Additionally, agents in MetaGPT communicate through role specialization, workflow management, and efficient sharing mechanisms like message pools and subscriptions.\n",
      "=== LLM Response ===\n",
      "In MetaGPT, the agent roles include Product Manager, Architect, Project Manager, Engineer, and QA Engineer. They work together in a sequential workflow following Standard Operating Procedures (SOPs) to develop software solutions efficiently. \n",
      "\n",
      "Agents in MetaGPT communicate with each other through structured communication interfaces, a shared message pool for direct information exchange, and a subscription mechanism based on role-specific interests. This ensures effective communication, task execution, and collaboration among the different agent roles.\n"
     ]
    }
   ],
   "source": [
    "response = agent.query(\n",
    "    \"Tell me about the agent roles in MetaGPT, \"\n",
    "    \"and then how they communicate with each other.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb38fab",
   "metadata": {
    "height": 30
   },
   "source": [
    "When we run a multi-step query like this, we want to make sure that we are actually able to trace the sources. So we use `response.source_nodes` to look at the content of these nodes. So we inspect the content of the first source node that is retrieved, which is the paper's first page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bcf74ec4-559f-4284-9ed0-817d26951c54",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_label: 1\n",
      "file_name: metagpt.pdf\n",
      "file_path: metagpt.pdf\n",
      "file_type: application/pdf\n",
      "file_size: 16911937\n",
      "creation_date: 2024-10-23\n",
      "last_modified_date: 2024-06-24\n",
      "\n",
      "Preprint\n",
      "METAGPT: M ETA PROGRAMMING FOR A\n",
      "MULTI -AGENT COLLABORATIVE FRAMEWORK\n",
      "Sirui Hong1∗, Mingchen Zhuge2∗, Jonathan Chen1, Xiawu Zheng3, Yuheng Cheng4,\n",
      "Ceyao Zhang4,Jinlin Wang1,Zili Wang ,Steven Ka Shing Yau5,Zijuan Lin4,\n",
      "Liyang Zhou6,Chenyu Ran1,Lingfeng Xiao1,7,Chenglin Wu1†,J¨urgen Schmidhuber2,8\n",
      "1DeepWisdom,2AI Initiative, King Abdullah University of Science and Technology,\n",
      "3Xiamen University,4The Chinese University of Hong Kong, Shenzhen,\n",
      "5Nanjing University,6University of Pennsylvania,\n",
      "7University of California, Berkeley,8The Swiss AI Lab IDSIA/USI/SUPSI\n",
      "ABSTRACT\n",
      "Remarkable progress has been made on automated problem solving through so-\n",
      "cieties of agents based on large language models (LLMs). Existing LLM-based\n",
      "multi-agent systems can already solve simple dialogue tasks. Solutions to more\n",
      "complex tasks, however, are complicated through logic inconsistencies due to\n",
      "cascading hallucinations caused by naively chaining LLMs. Here we introduce\n",
      "MetaGPT, an innovative meta-programming framework incorporating efficient\n",
      "human workflows into LLM-based multi-agent collaborations. MetaGPT en-\n",
      "codes Standardized Operating Procedures (SOPs) into prompt sequences for more\n",
      "streamlined workflows, thus allowing agents with human-like domain expertise\n",
      "to verify intermediate results and reduce errors. MetaGPT utilizes an assembly\n",
      "line paradigm to assign diverse roles to various agents, efficiently breaking down\n",
      "complex tasks into subtasks involving many agents working together. On col-\n",
      "laborative software engineering benchmarks, MetaGPT generates more coherent\n",
      "solutions than previous chat-based multi-agent systems. Our project can be found\n",
      "at https://github.com/geekan/MetaGPT.\n",
      "1 I NTRODUCTION\n",
      "Autonomous agents utilizing Large Language Models (LLMs) offer promising opportunities to en-\n",
      "hance and replicate human workflows. In real-world applications, however, existing systems (Park\n",
      "et al., 2023; Zhuge et al., 2023; Cai et al., 2023; Wang et al., 2023c; Li et al., 2023; Du et al., 2023;\n",
      "Liang et al., 2023; Hao et al., 2023) tend to oversimplify the complexities. They struggle to achieve\n",
      "effective, coherent, and accurate problem-solving processes, particularly when there is a need for\n",
      "meaningful collaborative interaction (Chen et al., 2024; Zhang et al., 2023; Dong et al., 2023; Zhou\n",
      "et al., 2023; Qian et al., 2023).\n",
      "Through extensive collaborative practice, humans have developed widely accepted Standardized\n",
      "Operating Procedures (SOPs) across various domains (Belbin, 2012; Manifesto, 2001; DeMarco &\n",
      "Lister, 2013). These SOPs play a critical role in supporting task decomposition and effective coor-\n",
      "dination. Furthermore, SOPs outline the responsibilities of each team member, while establishing\n",
      "standards for intermediate outputs. Well-defined SOPs improve the consistent and accurate exe-\n",
      "cution of tasks that align with defined roles and quality standards (Belbin, 2012; Manifesto, 2001;\n",
      "DeMarco & Lister, 2013; Wooldridge & Jennings, 1998). For instance, in a software company,\n",
      "Product Managers analyze competition and user needs to create Product Requirements Documents\n",
      "(PRDs) using a standardized structure, to guide the developmental process.\n",
      "Inspired by such ideas, we design a promising GPT -based Meta -Programming framework called\n",
      "MetaGPT that significantly benefits from SOPs. Unlike other works (Li et al., 2023; Qian et al.,\n",
      "2023), MetaGPT requires agents to generate structured outputs, such as high-quality requirements\n",
      "∗These authors contributed equally to this work.\n",
      "†Chenglin Wu (alexanderwu@fuzhi.ai) is the corresponding author, affiliated with DeepWisdom.\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(response.source_nodes[0].get_content(metadata_mode=\"all\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f21b5a5",
   "metadata": {
    "height": 30
   },
   "source": [
    "Calling `agent.query()` allows us to query the agent in a one-off manner, but does not preserve state. So now let's try maintaining conversation history over time. The agent is able to maintain chats in a conversational memory buffer. The memory module can be customized, but by default it's a flat list of items that's a rolling buffer depending on the size of the context window of the LLM. Therefore, when the agent decides to use a tool, it not only uses a current chat, but also the previous conversation history to take the next step or perform the next action. \n",
    "\n",
    "So instead of `agent.query()`, we will do `agent.chat()`. We will ask `Tell me about the evaluation datasets used.` Here we see that it uses the summary tool to ask `evaluation data sets used in MetaGPT`. And we see `The evaluation datasets used in MetaGPT include HumanEval, MBPP, and Software Dev.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b28c184-0b65-4e38-808e-d91a285aaefe",
   "metadata": {
    "height": 64,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Tell me about the evaluation datasets used.\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_metagpt with args: {\"input\": \"evaluation datasets used in MetaGPT\"}\n",
      "=== Function Output ===\n",
      "The evaluation datasets used in MetaGPT include HumanEval, MBPP, and the SoftwareDev dataset.\n",
      "=== LLM Response ===\n",
      "The evaluation datasets used in MetaGPT include HumanEval, MBPP, and the SoftwareDev dataset. These datasets are utilized to evaluate the performance and capabilities of MetaGPT in various tasks and domains.\n"
     ]
    }
   ],
   "source": [
    "response = agent.chat(\n",
    "    \"Tell me about the evaluation datasets used.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca37f3d1",
   "metadata": {
    "height": 30
   },
   "source": [
    "We will see an example of this ability to maintain conversation history because we will ask a follow-up question `Tell me the results over one of the above datasets.` Obviously, to know what the \"above datasets\" are, we need to have that stored in the conversation history somewhere. So let's run this and it is able to translate this query plus conversation history into a query on the vector tool. And it asks `results over HumanEval dataset`, which is one of the eval datasets used. And it is able to give us back a final answer. So we just provided a nice high level interface for interacting with an agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9586cef-21b5-4732-b95d-619462b4aaf6",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Tell me the results over one of the above datasets.\n",
      "=== Calling Function ===\n",
      "Calling function: vector_tool_metagpt with args: {\"query\": \"results over HumanEval dataset\", \"page_numbers\": [\"7\"]}\n",
      "=== Function Output ===\n",
      "MetaGPT achieved 85.9% and 87.7% Pass rates over the HumanEval dataset.\n",
      "=== LLM Response ===\n",
      "MetaGPT achieved pass rates of 85.9% and 87.7% over the HumanEval dataset. These results demonstrate the effectiveness of MetaGPT in performing tasks and generating responses in the evaluated dataset.\n"
     ]
    }
   ],
   "source": [
    "response = agent.chat(\"Tell me the results over one of the above datasets.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc4e983",
   "metadata": {},
   "source": [
    "## Lower-Level: Debuggability and Control"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197cf807",
   "metadata": {
    "height": 132
   },
   "source": [
    "The next section will show us capabilities that let us step through and control the agent in a much more granular fashion. This allows us to not only create a higher level research assistant over our RAG pipelines, but also debug and control it. Some of the benefits include greater debuggability into the execution of each step, as well as steerability by allowing us to inject user feedback. \n",
    "\n",
    "Having this low-level agent interface is powerful for 2 main reasons. \n",
    "\n",
    "1. The first is debuggability. If we are a developer building an agent, we want to have greater transparency and visibility into what's actually going on under the hood. If our agent isn't working the first time around, then we can go in and trace through the agent execution, see where it is failing, and try different inputs to see what actually modifies the agent execution into a correct response. \n",
    "\n",
    "\n",
    "2. Another reason is it enables richer UXs, where we are building a product experience around this core agentic capability. For instance, we want to listen to human feedback in the middle of agent execution, as opposed to only after the agent execution is complete for a given task. Then, we can create async queue, where we are able to listen to inputs from humans throughout the middle of agent execution. And if human input does come in, we can interrupt and modify the agent execution as it is going through a larger task, as opposed to having to wait until the agent task is complete."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83e98d8",
   "metadata": {
    "height": 98
   },
   "source": [
    "So we will start by defining our agent again through `FunctionCallingAgentWorker` as well as the `AgentRunner` setup. And then we will start using the low level API. We will first create a task object from the user query. And then we will running through steps for event interjecting our own. \n",
    "\n",
    "Now let's try executing a single step of this task. So let's create a task for this agent. And we will use the same question we used in the first part of this lesson `Tell me about the agent roles in MetaGPT, and then how they comunicate with each other.` This will return a task object which contains the input as well as additional state in the task object. \n",
    "\n",
    "And now let's try executing a single step of this task. We will call `agent.run_step(task.task_id)`. And the agent will execute a step of that task through the task ID and give us back a step output. We will see that it calls the summary tool with the input `agent roles in MetaGPT`, which is a very first part of this question. And then it stops there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "55abad72-b189-471a-accc-1621fd19c804",
   "metadata": {
    "height": 115,
    "tags": []
   },
   "outputs": [],
   "source": [
    "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
    "    [vector_tool, summary_tool], \n",
    "    llm=llm, \n",
    "    verbose=True\n",
    ")\n",
    "agent = AgentRunner(agent_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "18e911aa-4640-4f89-99c8-6cdf6aff07c6",
   "metadata": {
    "height": 81,
    "tags": []
   },
   "outputs": [],
   "source": [
    "task = agent.create_task(\n",
    "    \"Tell me about the agent roles in MetaGPT, \"\n",
    "    \"and then how they communicate with each other.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5eaf0b88-e03a-4dd9-91f6-f6f0c8758e64",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Tell me about the agent roles in MetaGPT, and then how they communicate with each other.\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_metagpt with args: {\"input\": \"agent roles in MetaGPT\"}\n",
      "=== Function Output ===\n",
      "The agent roles in MetaGPT include Product Manager, Architect, Project Manager, Engineer, and QA Engineer. Each role has specific responsibilities and expertise within the collaborative framework to efficiently complete complex software development tasks. The Product Manager conducts business-oriented analysis, the Architect translates requirements into system design components, the Project Manager distributes tasks, the Engineer executes code, and the QA Engineer formulates test cases to ensure code quality. Additionally, there are other team members involved in experiments, comparisons, figure creation, and overall project advising within the MetaGPT framework.\n"
     ]
    }
   ],
   "source": [
    "step_output = agent.run_step(task.task_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356333e2",
   "metadata": {
    "height": 30
   },
   "source": [
    "When we inspect the logs and the output of the agent, we see that the first part was actually executed. So we call `agent.get_completed-steps()` on the task ID, and we are able to look at `Num completed for task`. We see that 1 step has been completed, and this is a current output so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a8e77fac-8734-4071-a672-b3a9f30e2bf1",
   "metadata": {
    "height": 64,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num completed for task b5bf95f0-7747-49c2-9bd3-8fca8d15228b: 1\n",
      "The agent roles in MetaGPT include Product Manager, Architect, Project Manager, Engineer, and QA Engineer. Each role has specific responsibilities and expertise within the collaborative framework to efficiently complete complex software development tasks. The Product Manager conducts business-oriented analysis, the Architect translates requirements into system design components, the Project Manager distributes tasks, the Engineer executes code, and the QA Engineer formulates test cases to ensure code quality. Additionally, there are other team members involved in experiments, comparisons, figure creation, and overall project advising within the MetaGPT framework.\n"
     ]
    }
   ],
   "source": [
    "completed_steps = agent.get_completed_steps(task.task_id)\n",
    "print(f\"Num completed for task {task.task_id}: {len(completed_steps)}\")\n",
    "print(completed_steps[0].output.sources[0].raw_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111db0a8",
   "metadata": {
    "height": 64
   },
   "source": [
    "We can also take a look at any upcoming steps for the agent through `agent.get_upcoming_steps()`. We pass the task ID into the agent, and we can print out the number of upcoming steps for the task. We see that it is also 1, and we are able to look at a task step object with a task ID and an existing input. This input is currently `None`, because the agent actually just auto-generates action from the conversation history, and it doesn't need to generate an additional external input. \n",
    "\n",
    "The nice thing about this debugging interface is that we can pause execution now if we want to. We can take the intermediate results without completing the agent flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "db8de410-4b82-4daf-93da-28da57cbb0bb",
   "metadata": {
    "height": 64,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num upcoming steps for task b5bf95f0-7747-49c2-9bd3-8fca8d15228b: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TaskStep(task_id='b5bf95f0-7747-49c2-9bd3-8fca8d15228b', step_id='33980ddd-c5e3-4b41-9a8b-5e1605293aba', input=None, step_state={}, next_steps={}, prev_steps={}, is_ready=True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upcoming_steps = agent.get_upcoming_steps(task.task_id)\n",
    "print(f\"Num upcoming steps for task {task.task_id}: {len(upcoming_steps)}\")\n",
    "upcoming_steps[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54549e6",
   "metadata": {
    "height": 64
   },
   "source": [
    "Let's run the next 2 steps and actually try injecting user input. So let's ask `What about how agents share information?` as user input. This was not part of the original task query, but by injecting this, we can modify agent execution to give us back the result that we want. \n",
    "\n",
    "We see that we added the user message to memory, and that the next call here is `how agents share information in MetaGPT`. And we see from the function output that it is able to give back the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dc352582-2c17-46ef-ba80-0f571e920c3c",
   "metadata": {
    "height": 64,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: What about how agents share information?\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_metagpt with args: {\"input\": \"how agents share information in MetaGPT\"}\n",
      "=== Function Output ===\n",
      "Agents in MetaGPT share information through a structured communication protocol that includes a shared message pool. This pool allows agents to publish structured messages and subscribe to relevant messages based on their profiles. Additionally, agents can obtain directional information from other roles and public information from the environment. This structured communication interface enhances role communication efficiency within the framework. Agents also utilize a subscription mechanism based on their role-specific interests to extract relevant information, ensuring they receive only task-related information and avoid distractions from irrelevant details.\n"
     ]
    }
   ],
   "source": [
    "step_output = agent.run_step(\n",
    "    task.task_id, input=\"What about how agents share information?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05b5d53",
   "metadata": {
    "height": 64
   },
   "source": [
    "The overall task is roughly complete, and we just need to run one final step to synthesize the answer. To double check that this output is the last step, we just need to do `step_output.is_last`. \n",
    "\n",
    "So we are able to get back the answer about how agents and MetaGPT share information. And this is indeed the last step (ie. `is_last` returns `True` in the output)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "be80661f-81b1-45fc-b0ba-33a04dae849b",
   "metadata": {
    "height": 47,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== LLM Response ===\n",
      "In MetaGPT, agents share information through a structured communication protocol that includes a shared message pool. This pool allows agents to publish structured messages and subscribe to relevant messages based on their profiles. Agents can obtain directional information from other roles and public information from the environment. This structured communication interface enhances role communication efficiency within the framework. Agents also use a subscription mechanism based on their role-specific interests to extract relevant information, ensuring they receive only task-related information and avoid distractions from irrelevant details.\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "step_output = agent.run_step(task.task_id)\n",
    "print(step_output.is_last)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53655c13",
   "metadata": {
    "height": 30
   },
   "source": [
    "To translate this into an agent response, we just have to call `response = agent.finalize_response()`, and we will get back the final answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4496328c-e6d5-4722-a8df-78a73a441b3c",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = agent.finalize_response(task.task_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "601d1bed-78b2-4512-87ac-aec5ce5d8494",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant: In MetaGPT, agents share information through a structured communication protocol that includes a shared message pool. This pool allows agents to publish structured messages and subscribe to relevant messages based on their profiles. Agents can obtain directional information from other roles and public information from the environment. This structured communication interface enhances role communication efficiency within the framework. Agents also use a subscription mechanism based on their role-specific interests to extract relevant information, ensuring they receive only task-related information and avoid distractions from irrelevant details.\n"
     ]
    }
   ],
   "source": [
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4304b630",
   "metadata": {
    "height": 30
   },
   "source": [
    "So that's it for Lesson 3. We have learned both about the high level interface for an agent as well as a low level debugging interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044b6b62",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
