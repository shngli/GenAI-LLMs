{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46f32ff1",
   "metadata": {},
   "source": [
    "# Lesson 2: Tool Calling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb345ad0",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5bbf530-3f05-434c-a70f-ac2cc4b8f7aa",
   "metadata": {
    "height": 47,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from helper import get_openai_api_key\n",
    "OPENAI_API_KEY = get_openai_api_key()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4c06c95-e8b2-4574-b14d-685876aa1c47",
   "metadata": {
    "height": 47,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e53c064",
   "metadata": {},
   "source": [
    "## 1. Define a Simple Tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56de958b",
   "metadata": {
    "height": 98
   },
   "source": [
    "We will give a basic overview to tool calling. We will show how to define a tool interface from a Python function. And the LLM will automatically infer the parameters from the signature of the Python function using LlamaIndex abstractions. \n",
    "\n",
    "To illustrate this, let's define 2 toy calculator functions to show how tool calling works. We will define an `add()`and a `mystery()`. The core abstraction in LlamaIndex is the `FunctionTool`. This `FunctionTool` wraps any given Python function that we feed it. So we see that `FunctionTool` takes in both `add()` and `mystery()`. \n",
    "\n",
    "We see that `add()` and `mystery()` have type annotations for both the x and y variables, as well as the docstring. This is not just for stylistic purposes, this is actually important because these things will be used as a prompt for the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "071b717a-93cc-4332-b357-59a693359563",
   "metadata": {
    "height": 234,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.core.tools import FunctionTool\n",
    "\n",
    "def add(x: int, y: int) -> int:\n",
    "    \"\"\"Adds two integers together.\"\"\"\n",
    "    return x + y\n",
    "\n",
    "def mystery(x: int, y: int) -> int: \n",
    "    \"\"\"Mystery function that operates on top of two numbers.\"\"\"\n",
    "    return (x + y) * (x + y)\n",
    "\n",
    "\n",
    "add_tool = FunctionTool.from_defaults(fn=add)\n",
    "mystery_tool = FunctionTool.from_defaults(fn=mystery)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b806d6ee",
   "metadata": {
    "height": 98
   },
   "source": [
    "`FunctionTool` integrates natively with the function calling capabilities of many LLM models, including OpenAI. To pass the tools to an LLM, we have to import the LLM module and then call `predict_and_call()`. This code snippent imports the OpenAI module explicitly, and we see that the model is GPT 3.5 turbo.\n",
    "\n",
    "Then we call `predict_and_call()` on top of the LLM. `predict_and_call()` takes in a set of tools as well as an input prompt string, or a series of chat messages. Then it is able to make a decision on the tool to call, as well as call the tool itself to get back the final response. \n",
    "\n",
    "Here we see the intermediate steps calling mystery() with the arguments `x=2` and `y=9`, so we see that we call the right tools and infer the right parameters. And the output is `11*11 = 121`, which is the right answer. Note that this simple example is an expanded version of the router. The LLM not only picks the tool, but also decides what parameters to give to the tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4e62118-992b-4629-9022-be8c628209c1",
   "metadata": {
    "height": 166,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Calling Function ===\n",
      "Calling function: mystery with args: {\"x\": 2, \"y\": 9}\n",
      "=== Function Output ===\n",
      "121\n",
      "121\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
    "response = llm.predict_and_call(\n",
    "    [add_tool, mystery_tool], \n",
    "    \"Tell me the output of the mystery function on 2 and 9\", \n",
    "    verbose=True\n",
    ")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb8a835",
   "metadata": {},
   "source": [
    "## 2. Define an Auto-Retrieval Tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6589123f",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcdea238",
   "metadata": {
    "tags": []
   },
   "source": [
    "Let's use this key concept to define a slightly more sophisticated agentic layer on top of vector search. The LLM can choose vector search, and we can also get it to infer metadata filters, which is a structured list of tags that helps to return a more precise set of search results. \n",
    "\n",
    "We will use the MetaGPT PDF, and let's pay attention to the nodes themselves or the chunks, because we will take a look at the actual metadata attached to these chunks. \n",
    "\n",
    "Similar to the last lesson, we will use `SimpleDirectoryReader()` from LlamaIndex to load in the parsed representation of the PDF file. Next we will use `SentenceSplitter()` to split these documents into a set of even chunks, with a chunk size of 1024. Each node represents a chunk, and let's look at the content of an example chunk.\n",
    "\n",
    "To download this paper, below is the needed code:\n",
    "\n",
    "#!wget \"https://openreview.net/pdf?id=VtmBAGCN7o\" -O metagpt.pdf\n",
    "\n",
    "**Note**: The pdf file is included with this lesson. To access it, go to the `File` menu and select`Open...`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbe9326c-d7b3-452b-ae52-12f000157be4",
   "metadata": {
    "height": 64,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "# load documents\n",
    "documents = SimpleDirectoryReader(input_files=[\"metagpt.pdf\"]).load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5451f0a3-d0a6-4b5c-a337-8e1a343ff5f0",
   "metadata": {
    "height": 64,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "splitter = SentenceSplitter(chunk_size=1024)\n",
    "nodes = splitter.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237bb167",
   "metadata": {
    "height": 30
   },
   "source": [
    "So we will look at the content of the first chunk. We can do this from `node.get_content()`. We set `metadata_mode=\"all\"`, which is a special setting that enables us to print out the content of the node, and also the metadata attached to the document which is propagated to every node.\n",
    "\n",
    "Once we print this out, we get back a parsed representation of the paper's front page, we can also see the metadata attached at the very top. So this includes a few things: `page_label: 1`, `file_name: metagpt.pdf`, `file_type: pdf`, `file_size`, credit and the dates. \n",
    "\n",
    "We will pay special attention to the page labels because for instance, if we actually try a different node, we see that we get back a different page number. So we will add a page number annotation to every chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0fe0a9c-1f87-4ae7-a79e-7c3cf9c395ed",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_label: 1\n",
      "file_name: metagpt.pdf\n",
      "file_path: metagpt.pdf\n",
      "file_type: application/pdf\n",
      "file_size: 16911937\n",
      "creation_date: 2024-10-22\n",
      "last_modified_date: 2024-06-24\n",
      "\n",
      "Preprint\n",
      "METAGPT: M ETA PROGRAMMING FOR A\n",
      "MULTI -AGENT COLLABORATIVE FRAMEWORK\n",
      "Sirui Hong1∗, Mingchen Zhuge2∗, Jonathan Chen1, Xiawu Zheng3, Yuheng Cheng4,\n",
      "Ceyao Zhang4,Jinlin Wang1,Zili Wang ,Steven Ka Shing Yau5,Zijuan Lin4,\n",
      "Liyang Zhou6,Chenyu Ran1,Lingfeng Xiao1,7,Chenglin Wu1†,J¨urgen Schmidhuber2,8\n",
      "1DeepWisdom,2AI Initiative, King Abdullah University of Science and Technology,\n",
      "3Xiamen University,4The Chinese University of Hong Kong, Shenzhen,\n",
      "5Nanjing University,6University of Pennsylvania,\n",
      "7University of California, Berkeley,8The Swiss AI Lab IDSIA/USI/SUPSI\n",
      "ABSTRACT\n",
      "Remarkable progress has been made on automated problem solving through so-\n",
      "cieties of agents based on large language models (LLMs). Existing LLM-based\n",
      "multi-agent systems can already solve simple dialogue tasks. Solutions to more\n",
      "complex tasks, however, are complicated through logic inconsistencies due to\n",
      "cascading hallucinations caused by naively chaining LLMs. Here we introduce\n",
      "MetaGPT, an innovative meta-programming framework incorporating efficient\n",
      "human workflows into LLM-based multi-agent collaborations. MetaGPT en-\n",
      "codes Standardized Operating Procedures (SOPs) into prompt sequences for more\n",
      "streamlined workflows, thus allowing agents with human-like domain expertise\n",
      "to verify intermediate results and reduce errors. MetaGPT utilizes an assembly\n",
      "line paradigm to assign diverse roles to various agents, efficiently breaking down\n",
      "complex tasks into subtasks involving many agents working together. On col-\n",
      "laborative software engineering benchmarks, MetaGPT generates more coherent\n",
      "solutions than previous chat-based multi-agent systems. Our project can be found\n",
      "at https://github.com/geekan/MetaGPT.\n",
      "1 I NTRODUCTION\n",
      "Autonomous agents utilizing Large Language Models (LLMs) offer promising opportunities to en-\n",
      "hance and replicate human workflows. In real-world applications, however, existing systems (Park\n",
      "et al., 2023; Zhuge et al., 2023; Cai et al., 2023; Wang et al., 2023c; Li et al., 2023; Du et al., 2023;\n",
      "Liang et al., 2023; Hao et al., 2023) tend to oversimplify the complexities. They struggle to achieve\n",
      "effective, coherent, and accurate problem-solving processes, particularly when there is a need for\n",
      "meaningful collaborative interaction (Chen et al., 2024; Zhang et al., 2023; Dong et al., 2023; Zhou\n",
      "et al., 2023; Qian et al., 2023).\n",
      "Through extensive collaborative practice, humans have developed widely accepted Standardized\n",
      "Operating Procedures (SOPs) across various domains (Belbin, 2012; Manifesto, 2001; DeMarco &\n",
      "Lister, 2013). These SOPs play a critical role in supporting task decomposition and effective coor-\n",
      "dination. Furthermore, SOPs outline the responsibilities of each team member, while establishing\n",
      "standards for intermediate outputs. Well-defined SOPs improve the consistent and accurate exe-\n",
      "cution of tasks that align with defined roles and quality standards (Belbin, 2012; Manifesto, 2001;\n",
      "DeMarco & Lister, 2013; Wooldridge & Jennings, 1998). For instance, in a software company,\n",
      "Product Managers analyze competition and user needs to create Product Requirements Documents\n",
      "(PRDs) using a standardized structure, to guide the developmental process.\n",
      "Inspired by such ideas, we design a promising GPT -based Meta -Programming framework called\n",
      "MetaGPT that significantly benefits from SOPs. Unlike other works (Li et al., 2023; Qian et al.,\n",
      "2023), MetaGPT requires agents to generate structured outputs, such as high-quality requirements\n",
      "∗These authors contributed equally to this work.\n",
      "†Chenglin Wu (alexanderwu@fuzhi.ai) is the corresponding author, affiliated with DeepWisdom.\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(nodes[0].get_content(metadata_mode=\"all\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4adc4903",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_label: 4\n",
      "file_name: metagpt.pdf\n",
      "file_path: metagpt.pdf\n",
      "file_type: application/pdf\n",
      "file_size: 16911937\n",
      "creation_date: 2024-10-22\n",
      "last_modified_date: 2024-06-24\n",
      "\n",
      "Preprint\n",
      "Figure 2: An example of the communication protocol (left) and iterative programming with exe-\n",
      "cutable feedback (right). Left: Agents use a shared message pool to publish structured messages.\n",
      "They can also subscribe to relevant messages based on their profiles. Right : After generating the\n",
      "initial code, the Engineer agent runs and checks for errors. If errors occur, the agent checks past\n",
      "messages stored in memory and compares them with the PRD, system design, and code files.\n",
      "3 M ETAGPT: A M ETA-PROGRAMMING FRAMEWORK\n",
      "MetaGPT is a meta-programming framework for LLM-based multi-agent systems. Sec. 3.1 pro-\n",
      "vides an explanation of role specialization, workflow and structured communication in this frame-\n",
      "work, and illustrates how to organize a multi-agent system within the context of SOPs. Sec. 3.2\n",
      "presents a communication protocol that enhances role communication efficiency. We also imple-\n",
      "ment structured communication interfaces and an effective publish-subscribe mechanism. These\n",
      "methods enable agents to obtain directional information from other roles and public information\n",
      "from the environment. Finally, we introduce executable feedback—a self-correction mechanism for\n",
      "further enhancing code generation quality during run-time in Sec. 3.3.\n",
      "3.1 A GENTS IN STANDARD OPERATING PROCEDURES\n",
      "Specialization of Roles Unambiguous role specialization enables the breakdown of complex work\n",
      "into smaller and more specific tasks. Solving complex tasks or problems often requires the collab-\n",
      "oration of agents with diverse skills and expertise, each contributing specialized outputs tailored to\n",
      "specific issues.\n",
      "In a software company, a Product Manager typically conducts business-oriented analysis and derives\n",
      "insights, while a software engineer is responsible for programming. We define five roles in our\n",
      "software company: Product Manager, Architect, Project Manager, Engineer, and QA Engineer, as\n",
      "shown in Figure 1. In MetaGPT, we specify the agent’s profile, which includes their name, profile,\n",
      "goal, and constraints for each role. We also initialize the specific context and skills for each role.\n",
      "For instance, a Product Manager can use web search tools, while an Engineer can execute code, as\n",
      "shown in Figure 2. All agents adhere to the React-style behavior as described in Yao et al. (2022).\n",
      "Every agent monitors the environment ( i.e., the message pool in MetaGPT) to spot important ob-\n",
      "servations ( e.g.,, messages from other agents). These messages can either directly trigger actions or\n",
      "assist in finishing the job.\n",
      "Workflow across Agents By defining the agents’ roles and operational skills, we can establish\n",
      "basic workflows. In our work, we follow SOP in software development, which enables all agents to\n",
      "work in a sequential manner.\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "print(nodes[4].get_content(metadata_mode=\"all\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0facdb",
   "metadata": {
    "height": 132
   },
   "source": [
    "Next we will define a vector store index over these nodes. This will basically build a RAG indexing pipeline over these nodes. It will add an embedding for each node and it will get back a query engine. \n",
    "\n",
    "Differently from Lesson 1, we can actually try querying this RAG pipeline via metadata filters. Just to show how metadata filtering works, we import `MetadataFilters`, and then we simply specify a filter where the `page label=2`, in addition to `similarity_top_k=2`.\n",
    "\n",
    "Once we define this as a query engine, and we call some high level reuslts of MetaGPT, we get back a response. We will first look at the response string, which outlines the overall results of MetaGPT. \n",
    "\n",
    "Crucially we will take a look at the page number in the `source_nodes`. As we iterate through the source nodes, we can actually print out the metadata attached to these source nodes. We see that it is able to properly filter out the page numbers to only restrict the search to the set of pages where `page label = 2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7965cba-67b8-4cca-8e5f-2b0dbc96f6b0",
   "metadata": {
    "height": 81,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "vector_index = VectorStoreIndex(nodes)\n",
    "query_engine = vector_index.as_query_engine(similarity_top_k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "560f319c-8479-40c5-9b55-480fef98deb7",
   "metadata": {
    "height": 251,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.core.vector_stores import MetadataFilters\n",
    "\n",
    "query_engine = vector_index.as_query_engine(\n",
    "    similarity_top_k=2,\n",
    "    filters=MetadataFilters.from_dicts(\n",
    "        [\n",
    "            {\"key\": \"page_label\", \"value\": \"2\"}\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "\n",
    "response = query_engine.query(\n",
    "    \"What are some high-level results of MetaGPT?\", \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2da4042f-8fdb-4959-8760-86685c903cfd",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MetaGPT achieves a new state-of-the-art in code generation benchmarks with 85.9% and 87.7% in Pass@1, surpassing other popular frameworks like AutoGPT, LangChain, AgentVerse, and ChatDev. Additionally, MetaGPT demonstrates robustness and efficiency by achieving a 100% task completion rate in experimental evaluations, highlighting its effectiveness in handling higher levels of software complexity and offering extensive functionality.\n"
     ]
    }
   ],
   "source": [
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30bb264c-42e0-46f8-9d28-da11a8535960",
   "metadata": {
    "height": 47,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'page_label': '2', 'file_name': 'metagpt.pdf', 'file_path': 'metagpt.pdf', 'file_type': 'application/pdf', 'file_size': 16911937, 'creation_date': '2024-10-22', 'last_modified_date': '2024-06-24'}\n"
     ]
    }
   ],
   "source": [
    "for n in response.source_nodes:\n",
    "    print(n.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c392482",
   "metadata": {},
   "source": [
    "### Define the Auto-Retrieval Tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050b506d",
   "metadata": {
    "height": 64
   },
   "source": [
    "We will define a Python function that encapsulates the data retrieval tool. We define a function called `vector_query`, which takes in the query and the page numbers. This allows us to perform a vector search over an index, along with specifying page numbers as a metadata filter. \n",
    "\n",
    "At the very end, we define `vector_query_tool = FunctionTool.from_defaults()`. So we pass in `vector_query()` into `vector_query_tool`, which allows us to then use it with a language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2639e79b-f615-425b-85ea-8a279bb26dd0",
   "metadata": {
    "height": 608,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from llama_index.core.vector_stores import FilterCondition\n",
    "\n",
    "\n",
    "def vector_query(\n",
    "    query: str, \n",
    "    page_numbers: List[str]\n",
    ") -> str:\n",
    "    \"\"\"Perform a vector search over an index.\n",
    "    \n",
    "    query (str): the string query to be embedded.\n",
    "    page_numbers (List[str]): Filter by set of pages. Leave BLANK if we want to perform a vector search\n",
    "        over all pages. Otherwise, filter by the set of specified pages.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    metadata_dicts = [\n",
    "        {\"key\": \"page_label\", \"value\": p} for p in page_numbers\n",
    "    ]\n",
    "    \n",
    "    query_engine = vector_index.as_query_engine(\n",
    "        similarity_top_k=2,\n",
    "        filters=MetadataFilters.from_dicts(\n",
    "            metadata_dicts,\n",
    "            condition=FilterCondition.OR\n",
    "        )\n",
    "    )\n",
    "    response = query_engine.query(query)\n",
    "    return response\n",
    "    \n",
    "\n",
    "vector_query_tool = FunctionTool.from_defaults(\n",
    "    name=\"vector_tool\",\n",
    "    fn=vector_query\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c443a2",
   "metadata": {
    "height": 98
   },
   "source": [
    "So let's try calling this tool with GPT 3.5 turbo. We will find that the LLM is able to infer both the string and the metadata filters. We do `predict_and_call()` on the `vector_query_tool`, and ask the same question high level results of MetaGPT as described on page 2.\n",
    "\n",
    "We see that the LLM is able to formulate the right query, high level results of MetaGPT, as well as specify the page numbers, which is 2. And we get back the correct answer. Similarly as before, we verify the source nodes, and we see that there is 1 source node returned and the page label is 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2a408ace-cf25-425b-8248-7028ceabcd42",
   "metadata": {
    "height": 115,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Calling Function ===\n",
      "Calling function: vector_tool with args: {\"query\": \"high-level results of MetaGPT\", \"page_numbers\": [\"2\"]}\n",
      "=== Function Output ===\n",
      "MetaGPT achieves a new state-of-the-art (SoTA) in code generation benchmarks with 85.9% and 87.7% in Pass@1. It stands out in handling higher levels of software complexity and offering extensive functionality, demonstrating a 100% task completion rate in experimental evaluations.\n"
     ]
    }
   ],
   "source": [
    "llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "response = llm.predict_and_call(\n",
    "    [vector_query_tool], \n",
    "    \"What are the high-level results of MetaGPT as described on page 2?\", \n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6ec05565-6adf-4294-ba5c-b384220876ac",
   "metadata": {
    "height": 47,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'page_label': '2', 'file_name': 'metagpt.pdf', 'file_path': 'metagpt.pdf', 'file_type': 'application/pdf', 'file_size': 16911937, 'creation_date': '2024-10-22', 'last_modified_date': '2024-06-24'}\n"
     ]
    }
   ],
   "source": [
    "for n in response.source_nodes:\n",
    "    print(n.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef4dec0",
   "metadata": {},
   "source": [
    "## Let's add some other tools!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d72629",
   "metadata": {
    "height": 30
   },
   "source": [
    "Finally, we can bring in the summary tool from the router example in Lesson 1. And we can combine that with the vector tool to create this overall tool picking system. So this code just sets up a summary index over the same set of nodes, and wraps us in a summary tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "55dd32e5-e29f-42ed-839a-ca937fe4743e",
   "metadata": {
    "height": 268,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.core import SummaryIndex\n",
    "from llama_index.core.tools import QueryEngineTool\n",
    "\n",
    "summary_index = SummaryIndex(nodes)\n",
    "summary_query_engine = summary_index.as_query_engine(\n",
    "    response_mode=\"tree_summarize\",\n",
    "    use_async=True,\n",
    ")\n",
    "summary_tool = QueryEngineTool.from_defaults(\n",
    "    name=\"summary_tool\",\n",
    "    query_engine=summary_query_engine,\n",
    "    description=(\n",
    "        \"Useful if you want to get a summary of MetaGPT\"\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e074f2e8",
   "metadata": {
    "height": 30
   },
   "source": [
    "Now let's try tool calling again. So the LLM has a slightly harder task of picking the right tool in addition to inferring the function parameters. We ask \"What are the MetaGPT comparisons with ChatDev described on page 8?\" We see that it stills calls a vector tool with page number = 8, and it's able to give back the right answer. We can verify this by printing out the source nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4228ca7c-42a0-494b-987b-5a1c5c584536",
   "metadata": {
    "height": 98,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Calling Function ===\n",
      "Calling function: vector_tool with args: {\"query\": \"MetaGPT comparisons with ChatDev\", \"page_numbers\": [\"8\"]}\n",
      "=== Function Output ===\n",
      "MetaGPT outperforms ChatDev in various aspects such as executability, running times, token usage, code statistic, productivity, and human revision cost. It demonstrates superior performance in generating code efficiently and effectively compared to ChatDev.\n"
     ]
    }
   ],
   "source": [
    "response = llm.predict_and_call(\n",
    "    [vector_query_tool, summary_tool], \n",
    "    \"What are the MetaGPT comparisons with ChatDev described on page 8?\", \n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4aa3e8c1-a8c3-4c92-a0e4-5c081f91d966",
   "metadata": {
    "height": 47,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'page_label': '8', 'file_name': 'metagpt.pdf', 'file_path': 'metagpt.pdf', 'file_type': 'application/pdf', 'file_size': 16911937, 'creation_date': '2024-10-22', 'last_modified_date': '2024-06-24'}\n"
     ]
    }
   ],
   "source": [
    "for n in response.source_nodes:\n",
    "    print(n.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1d88f4",
   "metadata": {
    "height": 30
   },
   "source": [
    "Lastly, we can ask \"What is a summary of the paper?\" to show that the LLM can still pick the summary tool when necessary. And we see that it gives back the right response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "21906d47-7266-4479-bbb4-9f392d5c399b",
   "metadata": {
    "height": 98,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Calling Function ===\n",
      "Calling function: summary_tool with args: {\"input\": \"This paper discusses the impact of climate change on global food security and explores potential solutions to address this issue.\"}\n",
      "=== Function Output ===\n",
      "The paper does not discuss the impact of climate change on global food security or explore potential solutions to address this issue.\n"
     ]
    }
   ],
   "source": [
    "response = llm.predict_and_call(\n",
    "    [vector_query_tool, summary_tool], \n",
    "    \"What is a summary of the paper?\", \n",
    "    verbose=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
